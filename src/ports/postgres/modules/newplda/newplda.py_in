"""
@file newplda.py_in 

@brief Latent Dirichlet Allocation inference using collapsed Gibbs
sampling algorithm

@namespace newplda

Parallel LDA: Driver and auxiliary functions
"""

import plpy
import math
import time

"""
@brief This class defines a wrapper class for LDA training and prediction.
"""
class LDA:
    def __init__(
        self, madlib_schema, data_table, model_table, output_data_table,
        topic_num, iter_num, alpha, beta):

        self.madlib_schema = madlib_schema
        self.data_table = data_table
        self.model_table = model_table
        self.output_data_table = output_data_table

        self.topic_num = topic_num
        self.iter_num = iter_num
        self.alpha = alpha
        self.beta = beta

        self.work_table_0 = '__lda_work_table_0__'
        self.work_table_1 = '__lda_work_table_1__'
        self.doc_topic_count = '__lda_doc_topic_count__'
        self.word_topic_count = '__lda_word_topic_count__'

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.work_table_0))
        plpy.execute("""
            CREATE TEMP TABLE %s(
                distid              INT4, 
                docid               INT4, 
                wordid              INT4,
                topic_assignment    INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (distid)') 
            """ % (self.work_table_0)) 

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.work_table_1))
        plpy.execute("""
            CREATE TEMP TABLE %s(
                distid              INT4, 
                docid               INT4, 
                wordid              INT4,
                topic_assignment    INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (distid)') 
            """ % (self.work_table_1)) 

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.doc_topic_count))
        plpy.execute("""
            CREATE TEMP TABLE %s(
                docid       INT4, 
                topic_count INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (docid')) 
            """ % (self.doc_topic_count)) 

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.word_topic_count))
        plpy.execute("""
            CREATE TEMP TABLE %s(
                wordid       INT4, 
                topic_count INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (wordid')) 
            """ % (self.word_topic_count)) 

    def init_random(self):
        stime = time.time()
        plpy.info('initializing topics randomly...')

        plpy.execute('TRUNCATE TABLE %s' % (self.work_table_0));
        plpy.execute("""
            INSERT INTO %s
            SELECT
                gp_segment_id, docid, wordid, 
                %s.__newplda_random_assign(count, %s) 
            FROM %s data
            """ % (
                self.work_table_0, self.madlib_schema, self.topic_num,
                self.data_table)
            )

        etime = time.time()
        plpy.info('\t\ttime elapsed: %.2f seconds' % (etime - stime))

    def gen_model(self):
        plpy.info('\t\tgenerating models...')
        plpy.execute('DROP TABLE IF EXISTS %s' % (self.model_table))
        plpy.execute("""
            CREATE TABLE %s(
                wordid      INT4,
                topic_count INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (wordid)') 
            """ % (self.model_table)) 
        plpy.execute("""
            INSERT INTO %s SELECT wordid, topic_count FROM %s
            """ % (self.model_table, self.word_topic_count))

    def gen_output_data_table(self):
        plpy.info('\t\tgenerating output data table...')
        work_table_final = self.work_table_1
        if self.iter_num % 2 == 0:
            work_table_final = self.work_table_0

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.output_data_table))
        plpy.execute("""
            CREATE TABLE %s(
                docid               INT4, 
                wordid              INT4,
                topic_assignment    INT4[]
                ) 
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (docid)') 
            """ % (self.output_data_table)) 
        plpy.execute("""
            INSERT INTO %s SELECT docid, wordid, topic_assignment FROM %s
            """ % (self.output_data_table, work_table_final))

    def comp_ll(self):
        stime = time.time()
        plpy.info('\t\tcomputing log likelihood...')

        ll_d = plpy.execute("""
            SELECT
                %s.__newplda_loglikelihood_agg(topic_count, %f) ll
            FROM
                %s dz
            """  % (self.madlib_schema, self.alpha, self.doc_topic_count))
        ll_w = plpy.execute("""
            SELECT
                %s.__newplda_loglikelihood_agg(topic_count, %f) ll
            FROM
                %s wz
            """  % (self.madlib_schema, self.beta, self.word_topic_count))
        ll = ll_d[0]['ll'] + ll_w[0]['ll']

        etime = time.time()
        plpy.info('\t\t\tLL=%.2f, time elapsed: %.2f seconds' % (ll, etime - stime))
        return ll

    def get_word_topic_count(self, work_table):
        plpy.execute("""TRUNCATE TABLE %s""" % (self.word_topic_count))
        plpy.execute("""
            INSERT INTO
                %s
            SELECT 
                wordid, 
                %s.__newplda_count_topic_agg(topic_assignment, %d)
            FROM
                %s 
            GROUP BY wordid
            """ % (
                self.word_topic_count, self.madlib_schema, self.topic_num,
                work_table))

    def get_doc_topic_count(self, work_table):
        plpy.execute("""TRUNCATE TABLE %s""" % (self.doc_topic_count))
        plpy.execute("""
            INSERT INTO
                %s
            SELECT 
                docid, 
                %s.__newplda_count_topic_agg(topic_assignment, %d)
            FROM
                %s
            GROUP BY docid
            """ % (
                self.doc_topic_count, self.madlib_schema, self.topic_num,
                work_table))

    def iterate(self, it, is_train=True):  
        stime = time.time()
        plpy.info('iteration [%d]....' % (it))

        work_table_in = self.work_table_0
        if it % 2 == 0 :
            work_table_in = self.work_table_1
        work_table_out = self.work_table_1
        if it % 2 == 0 :
            work_table_out = self.work_table_0

        plpy.execute('TRUNCATE TABLE %s' % (work_table_out))
        plpy.execute("""
            INSERT INTO %s
            SELECT  distid, dwz.docid, dwz.wordid,
                %s.__newplda_gibbs_sample(
                    topic_assignment, dz.topic_count, wz.topic_count,
                    corpus_topic_count::INT4[], %f, %f) 
            FROM
                %s dz,
                %s wz,
                (
                    SELECT
                        sum(topic_count) corpus_topic_count
                    FROM
                        %s wz
                ) cz,
                %s dwz
            WHERE
                dwz.docid = dz.docid AND
                dwz.wordid = wz.wordid
            """ % (
                work_table_out, 
                self.madlib_schema, self.alpha, self.beta,
                self.doc_topic_count,
                self.word_topic_count,
                self.word_topic_count,
                work_table_in)
            )

        self.get_doc_topic_count(work_table_out)
        if(is_train):
            self.get_word_topic_count(work_table_out)

        etime = time.time()
        plpy.info('\t\ttime elapsed: %.2f seconds' % (etime - stime))

    def train(self):
        stime = time.time()
        plpy.info('start the training process...')

        self.init_random()
        self.get_doc_topic_count(self.work_table_0)
        self.get_word_topic_count(self.work_table_0)
        self.comp_ll()

        for it in range(1, self.iter_num + 1):
            self.iterate(it)
            self.comp_ll()

        self.gen_model()
        self.gen_output_data_table()

        etime = time.time()
        plpy.info('finished, time elapsed: %.2f seconds' % (etime - stime))

    def predict(self):
        stime = time.time()
        plpy.info('start the prediction process...')
        word_topic_count = self.word_topic_count

        self.init_random()
        self.get_doc_topic_count(self.work_table_0)
        self.word_topic_count = self.model_table
        self.comp_ll()

        for it in range(1, self.iter_num + 1):
            self.iterate(it, is_train = False)
            self.comp_ll()

        self.gen_output_data_table()
        self.word_topic_count = word_topic_count

        etime = time.time()
        plpy.info('finished, time elapsed: %.2f seconds' % (etime - stime))


"""
@brief This function provides the entry for the LDA training process. 
@param madlib_schema        MDALib schema
@param data_table           Training data table
@param voc_size             Size of vocabulary
@param topic_num            Number of topics
@param iter_num             Number of iterations
@param alpha                Dirichlet parameter for per-document topic multinomial
@param beta                 Dirichlet parameter for per-topic word multinomial
@param model_table          Learned model table
@param output_data_table    Output data table
"""
def lda_run(
    madlib_schema, train_table, model_table, output_data_table, topic_num,
    iter_num, alpha, beta, is_train = True):
    __assert(
        train_table is not None and
        model_table is not None and
        output_data_table is not None and
        topic_num is not None and
        iter_num is not None and
        alpha is not None and
        beta is not None,
        "None of parameters could be null")

    __assert(
        train_table != '',  
        'The train_table should be a non-empty string')
    __assert(
        model_table != '',
        'The model_table should be a non-empty string')
    __assert(
        output_data_table != '',
        'The output_data_table should be a non-empty string')
    __assert(
        topic_num > 0, 
        'The topic_num should be a natural number')
    __assert(
        iter_num >= 0,
        'The iter_num should be a non-negative integer')
    __assert(
        alpha > 0, 
        'The alpha should be a non-negative real')
    __assert(
        beta > 0, 
        'The beta should be a non-negative real')
    if(not is_train):
        __check_model_table(model_table) 
  
    lda = LDA(
        madlib_schema, train_table, model_table, output_data_table, topic_num,
        iter_num, alpha, beta);
    if(is_train):
        lda.train()
    else:
        lda.predict()

"""
@brief Get the per-topic description by top-k words
@param model_table  The model table generated by the training process
@param vocab_table  The vocabulary table
@param desc_table   The output table for storing the per-topic word description
@param top_k        The top k words for topic description
"""
def get_topic_desc(
    madlib_schema, model_table, vocab_table, desc_table, top_k):
    __assert(
        model_table is not None and
        vocab_table is not None and
        desc_table is not None and
        top_k is not None,
        'None of the parameters can be null')

    __assert(
        model_table != '' and
        vocab_table != '' and
        desc_table != '',
        'No table name can be empty string')

    __assert(
        top_k > 0,
        'The top_k should be a positive integer')

    __check_model_table(model_table) 
    __check_vocab_table(vocab_table)

    rv = plpy.execute("""
        SELECT array_upper(topic_count, 1) topic_num FROM %s LIMIT 1
        """ % (model_table))
    topic_num = rv[0]['topic_num']

    plpy.execute('DROP TABLE IF EXISTS __topic_word_count__')
    plpy.execute("""
        CREATE TABLE __topic_word_count__
        (
            topicid INT4, 
            wordid  INT4, 
            count   INT4
        )
        m4_ifdef(
            `__GREENPLUM__', 
            `WITH (APPENDONLY=TRUE, COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
            (topicid)') 
        """)
    for topic in xrange(1, topic_num + 1):
        plpy.execute("""
            INSERT INTO __topic_word_count__
            SELECT
                %d topicid, wordid, topic_count[%d] word_count
            FROM
                %s model
            """ % (topic, topic, model_table))

    plpy.execute('DROP TABLE IF EXISTS %s' % (desc_table))
    plpy.execute("""
        CREATE TABLE %s 
        (
            topicid INT4, 
            wordid  INT4, 
            prob    FLOAT8, 
            word    TEXT
        )
        m4_ifdef(
            `__GREENPLUM__', 
            `WITH (APPENDONLY=TRUE, COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
            (topicid)') 
        """ % (desc_table))

    plpy.execute("""
        INSERT INTO %s
        SELECT
            topicid, t1.wordid, 
            (count::FLOAT8 + 0.01)/(total_count + voc_size * 0.01) prob,
            word
        FROM
        (
            SELECT
                topicid, wordid, count, 
                sum(count) OVER (PARTITION BY topicid) total_count, 
                count() OVER (PARTITION BY topicid) voc_size, 
                rank() OVER(PARTITION BY topicid ORDER BY count DESC) r 
            FROM
                __topic_word_count__ 
        ) t1,
        %s voc
        WHERE
            t1.wordid = voc.wordid AND
            t1.r <= %d
        """ % (desc_table, vocab_table, top_k))

"""
@brief Normalize the vector using L1 norm with smoothing
@param vector   The vector to be normalized
@param smooth   The smoothing parameter
@return         The normalized vector
"""
def l1_norm_with_smoothing(vector, smooth):
    __assert(
        vector is not None and
        smooth is not None,
        'neither vector nor smooth could be null')
    norm = sum(map(lambda r: abs(r), vector))
    norm += abs(smooth) * len(vector)
    return map(lambda r: float(r + abs(smooth)) / norm, vector)

"""
@brief Return the index of elements in a sorted order
@param vector   The array to be sorted
@return         The index of elements
"""
def index_sort(vector):
    __assert(vector is not None,
        'The input vector is null')

    dim = len(vector)
    idx = range(dim)
    idx.sort(key = lambda r: vector[r])

    return map(lambda r: r + 1, idx)

"""
@brief if the given condition is false, then raise an error with the message
@param condition    the condition to be asserted
@param msg          the error message to be reported
"""
def __assert(condition, msg):
    if not condition:
        plpy.error(msg)

"""
@brief if the given condition is false, then raise an warning with the message
@param condition    the condition to be asserted
@param msg          the error message to be reported
"""
def __warn(condition, msg):
    if not condition:
        plpy.warning(msg)

"""
@brief Check the structure of the data table
@param data_table   The data table name
"""
def __check_data_table(data_table):
    try:
        rv = plpy.execute("""
            SELECT count(*) cnt FROM pg_attribute 
            WHERE 
                attrelid = '%s'::regclass AND
                ((atttypid = 'INT4'::regtype AND attname = 'docid') OR
                (atttypid = 'INT4'::regtype AND attname = 'wordid') OR
                (atttypid = 'INT4'::regtype AND attname = 'count'))
            """ % (data_table))
        __assert(
           rv[0]['cnt'] == 3, 
           """
           The docid::INT4, wordid::INT4, count::INT4 should exisit in the %s
           """ % (data_table))
    except:
        plpy.error("""
           The %s must exisit and should have docid, wordid, and count
           columns
        """ % (data_table))

"""
@brief Check the validity of the vocabulary table
@param vocab_table   The vocabulary table name
"""
def __check_vocab_table(vocab_table):
    try:
        rv = plpy.execute("""
            SELECT count(*) cnt FROM pg_attribute 
            WHERE 
                attrelid = '%s'::regclass AND
                ((atttypid = 'INT4'::regtype AND attname = 'wordid') OR
                (atttypid = 'text'::regtype AND attname = 'word'))
            """ % (vocab_table))
        __assert(
           rv[0]['cnt'] == 2, 
           'The wordid::INT4, word::text should exisit in %s' % (vocab_table))
    except:
        plpy.error("""
            The %s must exisit and should have wordid and word columnes
        """ % (vocab_table))

"""
@brief Check the validity of the model table
@param model_table  Model table name
"""
def __check_model_table(model_table):
    try:
        rv = plpy.execute("""
            SELECT count(*) cnt 
            FROM pg_attribute 
            WHERE 
               attrelid = '%s'::regclass AND
               ((atttypid = 'INT4'::regtype AND attname = 'wordid') OR
               (atttypid = 'INT4[]'::regtype AND attname = 'topic_count')) 
           """ % (model_table))

        __assert(
            rv[0]['cnt'] == 2, 
            """
                The wordid and topic_count should exisit in the %s
            """ % (model_table))
    except:
        plpy.error("""
            The %s must exisit and should have wordid, topic_count columns.
        """ % (model_table))
       
    rv = plpy.execute("""
        SELECT 
            max(topic_num) maxz, min(topic_num) minz
        FROM
        (
            SELECT  
                array_upper(topic_count, 1) topic_num
            FROM %s
        ) t1
        """ % (model_table))

    __assert(
        len(rv) == 1, 
        'The %s should not be empty' % (model_table))

    __assert(
        rv[0]['maxz'] == rv[0]['minz'], 
        'Inconsistent dimension in %s.' % (model_table))
    __assert(
        rv[0]['maxz'] > 0, 
        'Invaid topic counts in %s.' % (model_table))
